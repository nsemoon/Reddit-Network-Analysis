{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## A Network Analysis On The Subreddit r/mentalhealth:\n",
        "**Who leads the conversation on mental health?**"
      ],
      "metadata": {
        "id": "hHAIMDFli6vn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "19xlNZ2Vd_jI"
      },
      "outputs": [],
      "source": [
        "!pip install praw\n",
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import praw\n",
        "import networkx as nx\n",
        "from google.colab import userdata\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "import re\n",
        "import logging\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "logging.getLogger(\"praw\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"prawcore\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "XpYhie_7eHWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reddit = (\n",
        "    userdata.get('client_id'),\n",
        "    userdata.get('client_secret'),\n",
        "    userdata.get('user_agent')\n",
        ")\n"
      ],
      "metadata": {
        "id": "e_9dIgV2eJzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SUBREDDIT_NAME = \"mentalhealth\"\n",
        "TARGET_COMMENTS = 5000\n",
        "TARGET_USERS = 5000\n",
        "TOP_POSTS_LIMIT = 50\n",
        "NEW_POSTS_LIMIT = 50\n",
        "\n",
        "g = nx.DiGraph()\n",
        "unique_users = set()\n",
        "total_comments = 0\n",
        "comment_data = []\n",
        "\n",
        "# Unpack your credentials\n",
        "client_id, client_secret, user_agent = reddit\n",
        "\n",
        "# Initialize Reddit instance\n",
        "reddit = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=client_secret,\n",
        "    user_agent=user_agent\n",
        ")\n",
        "\n",
        "subreddit = reddit.subreddit(SUBREDDIT_NAME)\n",
        "\n",
        "def process_submission(submission):\n",
        "    global total_comments\n",
        "\n",
        "    submission.comments.replace_more(limit=0)\n",
        "    comment_lookup = {}\n",
        "\n",
        "    # Map post author\n",
        "    if submission.author:\n",
        "        comment_lookup[submission.id] = str(submission.author)\n",
        "        unique_users.add(str(submission.author))\n",
        "        g.add_node(str(submission.author))\n",
        "\n",
        "    for comment in submission.comments.list():\n",
        "        if comment.author is None or comment.body is None:\n",
        "            continue\n",
        "\n",
        "        author = str(comment.author)\n",
        "        comment_id = comment.id\n",
        "        parent_id = comment.parent_id.split(\"_\")[1]\n",
        "\n",
        "        comment_lookup[comment_id] = author\n",
        "        unique_users.add(author)\n",
        "        g.add_node(author)\n",
        "\n",
        "        parent_author = comment_lookup.get(parent_id)\n",
        "        if parent_author and parent_author != author:\n",
        "            if g.has_edge(author, parent_author):\n",
        "                g[author][parent_author]['weight'] += 1\n",
        "            else:\n",
        "                g.add_edge(author, parent_author, weight=1)\n",
        "\n",
        "        # Store comment\n",
        "        comment_data.append({\"user\": author, \"comment\": comment.body})\n",
        "        total_comments += 1\n",
        "\n",
        "        # Stop if targets met\n",
        "        if total_comments >= TARGET_COMMENTS or len(unique_users) >= TARGET_USERS:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "# Collect comments from top posts\n",
        "for submission in subreddit.top(limit=TOP_POSTS_LIMIT):\n",
        "    if process_submission(submission):\n",
        "        break\n",
        "\n",
        "# If still below targets, collect from new posts\n",
        "if total_comments < TARGET_COMMENTS and len(unique_users) < TARGET_USERS:\n",
        "    for submission in subreddit.new(limit=NEW_POSTS_LIMIT):\n",
        "        if process_submission(submission):\n",
        "            break\n",
        "\n",
        "# Create DataFrame once\n",
        "df = pd.DataFrame(comment_data)\n",
        "\n",
        "print(\"\\n=== Collection Complete ===\")\n",
        "print(f\"Total comments collected: {len(df)}\")\n",
        "print(f\"Unique users collected: {len(unique_users)}\")\n",
        "print(f\"Total nodes in graph: {len(g.nodes)}\")\n",
        "print(f\"Total edges in graph: {len(g.edges)}\")\n",
        "\n",
        "# Save graph and DataFrame\n",
        "nx.write_graphml(g, \"reddit_mentalhealth_combined.graphml\")\n",
        "df.to_csv(\"reddit_mentalhealth_comments.csv\", index=False)\n",
        "print(\"Graph and comments DataFrame saved.\")\n"
      ],
      "metadata": {
        "id": "HSBOj1m22tR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Measuring Influence Based on Degree Centrality, PageRank, and Betweenness Centrality**"
      ],
      "metadata": {
        "id": "U2HsPOV12wOM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_k = 10\n",
        "centrality_degree = nx.degree_centrality(g)\n",
        "\n",
        "print(\"\\nTop 10 users by degree centrality:\")\n",
        "for user in sorted(centrality_degree, key=centrality_degree.get, reverse=True)[:top_k]:\n",
        "    # No 'name' attribute assumed, so print username directly\n",
        "    print(f\"{user}: Degree Centrality = {centrality_degree[user]:.4f}\")"
      ],
      "metadata": {
        "id": "8PMLdUgHe5Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pagerank = nx.pagerank(g, weight='weight')\n",
        "print(\"\\nTop 10 users by PageRank:\")\n",
        "for user in sorted(pagerank, key=pagerank.get, reverse=True)[:top_k]:\n",
        "    print(f\"{user}: PageRank = {pagerank[user]:.4f}\")"
      ],
      "metadata": {
        "id": "okNmMLsAfqYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "btw = nx.betweenness_centrality(g,k=10)\n",
        "print(\"\\nTop 10 users by Betweenness:\")\n",
        "for user in sorted(btw, key=btw.get, reverse=True)[:top_k]:\n",
        "        print(f\"{user}: Betweenness = {btw[user]:.4f}\")"
      ],
      "metadata": {
        "id": "-EAgTKAxgM9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extension: Answering an additional question for our stakeholder\n",
        "**“What sub-communities of users exist in the subreddit r/mentalhealth based on the semantic patterns in their comments?”**"
      ],
      "metadata": {
        "id": "KtyOzRgEiqcd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**aggregate comments per user**"
      ],
      "metadata": {
        "id": "sVnwqbfyho4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"reddit_mentalhealth_comments.csv\")\n",
        "\n",
        "# Aggregate all comments per user\n",
        "user_text = (\n",
        "    df.groupby(\"user\")[\"comment\"]\n",
        "      .apply(lambda x: \" \".join(x))\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "print(\"Users:\", len(user_text))\n"
      ],
      "metadata": {
        "id": "oBRZZo4SZZA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**create semantic embeddings**"
      ],
      "metadata": {
        "id": "ZvIUYDKkhtpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "embeddings = model.encode(\n",
        "    user_text[\"comment\"].tolist(),\n",
        "    show_progress_bar=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "vxux0ojCeZTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**dimensionality reduction (UMAP)**\n",
        "\n",
        "We use this to reduce the noise from the Reddit comments"
      ],
      "metadata": {
        "id": "7swk96b7hxJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "\n",
        "reducer = umap.UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    n_components=2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "embedding_2d = reducer.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "cDksLOTFe8OM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**similarity-based clustering**\n",
        "\n",
        "See the visualization at the very bottom. The elbow method was used to determine the number of clusters (k)"
      ],
      "metadata": {
        "id": "mKFfZ-K1h7n5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)\n",
        "clusters = kmeans.fit_predict(embeddings)\n",
        "\n",
        "user_text[\"cluster\"] = clusters\n",
        "user_text[\"x\"] = embedding_2d[:, 0]\n",
        "user_text[\"y\"] = embedding_2d[:, 1]"
      ],
      "metadata": {
        "id": "kG54KbRwfsid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for c in sorted(user_text[\"cluster\"].unique()):\n",
        "    print(f\"\\nCluster {c}\")\n",
        "    print(user_text[user_text[\"cluster\"] == c][\"user\"].head(5).tolist())"
      ],
      "metadata": {
        "id": "-fOp05DAghqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Name Clusters Meaningfully**\n",
        "\n",
        "Look at what each these users talk about"
      ],
      "metadata": {
        "id": "DXojCuG9hfn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Get cluster centers\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "n_clusters = 4 # Defined from the KMeans initialization\n",
        "\n",
        "def closest_users_to_center(X, labels, center_idx, users, top_n=5):\n",
        "    cluster_points = X[labels == center_idx]\n",
        "    cluster_users = users[labels == center_idx]\n",
        "    center = centers[center_idx]\n",
        "\n",
        "    distances = np.linalg.norm(cluster_points - center, axis=1)\n",
        "    closest_idx = np.argsort(distances)[:top_n]\n",
        "\n",
        "    return cluster_users.iloc[closest_idx]\n",
        "\n",
        "for c in range(n_clusters):\n",
        "    print(f\"\\nCluster {c} representative users:\")\n",
        "    print(closest_users_to_center(embeddings, clusters, c, user_text[\"user\"]))"
      ],
      "metadata": {
        "id": "H3j-8XaOhjLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Representative Users Per Cluster: Identify Highest PageRank Users**"
      ],
      "metadata": {
        "id": "JuCgdBSoitzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "pagerank_df = pd.DataFrame.from_dict(pagerank, orient=\"index\", columns=[\"pagerank\"])\n",
        "pagerank_df[\"user\"] = pagerank_df.index\n",
        "\n",
        "final_df = user_text.merge(pagerank_df, on=\"user\", how=\"left\")\n",
        "\n",
        "for cluster_id in final_df[\"cluster\"].unique():\n",
        "    print(f\"\\nCluster {cluster_id} – top influential users:\")\n",
        "    print(\n",
        "        final_df[final_df[\"cluster\"] == cluster_id]\n",
        "        .sort_values(\"pagerank\", ascending=False)\n",
        "        .head(5)[[\"user\", \"pagerank\"]]\n",
        "    )"
      ],
      "metadata": {
        "id": "mEfFn5ljje-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Top distinguishing keywords per cluster**\n",
        "\n",
        "Interpret the clusters and assign label names"
      ],
      "metadata": {
        "id": "QCBvZ0AVnBKA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "cluster_keywords = {}\n",
        "\n",
        "for cluster_id in final_df[\"cluster\"].unique():\n",
        "    cluster_text = final_df[final_df[\"cluster\"] == cluster_id][\"comment\"].tolist()\n",
        "\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        stop_words=\"english\",\n",
        "        max_features=1000,\n",
        "        min_df=3\n",
        "    )\n",
        "\n",
        "    X = vectorizer.fit_transform(cluster_text)\n",
        "    tfidf_scores = X.mean(axis=0).A1\n",
        "\n",
        "    keywords = pd.Series(\n",
        "        tfidf_scores,\n",
        "        index=vectorizer.get_feature_names_out()\n",
        "    ).sort_values(ascending=False).head(10)\n",
        "\n",
        "    cluster_keywords[cluster_id] = keywords\n",
        "\n",
        "# Print keywords\n",
        "for k, words in cluster_keywords.items():\n",
        "    print(f\"\\nCluster {k} top keywords:\")\n",
        "    print(words.index.tolist())"
      ],
      "metadata": {
        "id": "m7SklRmMlJYO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Merge clusters with PageRank**"
      ],
      "metadata": {
        "id": "AnlFZEyNn0fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_influence = (\n",
        "    final_df.groupby(\"cluster\")[\"pagerank\"]\n",
        "    .mean()\n",
        "    .sort_values(ascending=False)\n",
        ")\n",
        "\n",
        "print(cluster_influence)\n"
      ],
      "metadata": {
        "id": "Kz2uFMR-lxhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert to a DataFrame for easier plotting\n",
        "plot_df = cluster_influence.reset_index()\n",
        "plot_df.columns = ['cluster', 'mean_pagerank']\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='cluster', y='mean_pagerank', data=plot_df, palette='viridis')\n",
        "\n",
        "plt.title(\"Mean PageRank by Cluster\")\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Mean PageRank\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qwUnfOyZtPkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Load the graph you created earlier\n",
        "g = nx.read_graphml(\"reddit_mentalhealth_combined.graphml\")\n",
        "\n",
        "# Ensure all users in your DataFrame are in the graph\n",
        "for idx, row in final_df.iterrows():\n",
        "    user = row['user']\n",
        "    if user in g.nodes:\n",
        "        g.nodes[user]['cluster'] = int(row['cluster'])\n",
        "        g.nodes[user]['pagerank'] = float(row['pagerank'])\n",
        "\n",
        "nx.write_graphml(g, \"reddit_mentalhealth_clusters.graphml\")\n",
        "print(\"Graph saved with clusters and PageRank for Gephi visualization.\")\n"
      ],
      "metadata": {
        "id": "Mn14tV1Ip6gL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualizations to support the analysis**"
      ],
      "metadata": {
        "id": "FEd6gJdnqP_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_sizes = final_df[\"cluster\"].value_counts().sort_index()\n",
        "\n",
        "plt.figure()\n",
        "plt.bar(cluster_sizes.index.astype(str), cluster_sizes.values)\n",
        "plt.xlabel(\"Cluster\")\n",
        "plt.ylabel(\"Number of Users\")\n",
        "plt.title(\"Number of Users per Cluster\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YzERb0uLhGwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure()\n",
        "plt.scatter(\n",
        "user_text[\"x\"],\n",
        "user_text[\"y\"],\n",
        "user_text[\"cluster\"]\n",
        ")\n",
        "plt.xlabel(\"Semantic Dimension 1\")\n",
        "plt.ylabel(\"Semantic Dimension 2\")\n",
        "plt.title(\"Semantic Clustering of Users in r/mentalhealth\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xEfJ37Y-lr3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Justification for KMeans**"
      ],
      "metadata": {
        "id": "wSH5ZOBVqLYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate inertia for a range of k values\n",
        "k_range = range(1, 10)\n",
        "inertias = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans_model = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_model.fit(embeddings)\n",
        "    inertias.append(kmeans_model.inertia_)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(list(k_range), inertias, marker='o')\n",
        "plt.xlabel(\"Number of clusters (k)\")\n",
        "plt.ylabel(\"Inertia\")\n",
        "plt.title(\"Elbow Method for Selecting Number of Clusters\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XGaq4ioCmnYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def remove_widgets_metadata(notebook_path: str):\n",
        "    # Open and parse the notebook JSON\n",
        "    with open(notebook_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        nb = json.load(f)\n",
        "\n",
        "    # Remove top-level metadata.widgets if it exists\n",
        "    if \"metadata\" in nb and \"widgets\" in nb[\"metadata\"]:\n",
        "        del nb[\"metadata\"][\"widgets\"]\n",
        "        print(f\"Removed metadata.widgets from {notebook_path}\")\n",
        "\n",
        "    # Optional: Write the cleaned notebook back to the same file\n",
        "    with open(notebook_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(nb, f, indent=2)\n",
        "\n",
        "# Example usage\n",
        "remove_widgets_metadata(\"INST414_Final_Project (1).ipynb\")\n"
      ],
      "metadata": {
        "id": "GA6J5rSS07pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ut-pYS045PPU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}